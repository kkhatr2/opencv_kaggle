{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c5326-c045-4ff3-b7fd-d9eee1a44665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "from PIL import Image as im\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics import Accuracy, MeanMetric\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab0e0d0-424c-4be9-9bfe-ee330038ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_root, img_size, batch_size=32, train_split=0.8, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.root = data_root\n",
    "        self.metadata = self.get_metadata()\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_split = train_split\n",
    "        self.num_workers = num_workers\n",
    "        transform_params = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "        self.train_mean = transform_params.mean\n",
    "        self.train_std = transform_params.std\n",
    "        self.common_transforms = transforms.Compose([\n",
    "            transforms.Resize(transform_params.resize_size, \n",
    "                              interpolation=transform_params.interpolation),\n",
    "            transforms.CenterCrop(transform_params.crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.train_mean,\n",
    "                                 std=self.train_std),\n",
    "            \n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def prepare_data_per_node(self):\n",
    "        # There is nothing to prepare.\n",
    "        # The data should have already been downloaded and\n",
    "        # extracted in the data_root that is passed to initiaize this class\n",
    "        print(os.path.abspath(self.root))\n",
    "        \n",
    "    \n",
    "    def setup(self, stage='fit'):\n",
    "        data_dict = self.get_img_path_labels(stage)\n",
    "        imgs_ct = 100 #len(data_dict['image_path'])\n",
    "        imgs_tensor = torch.empty([imgs_ct, 3, self.img_size, self.img_size])\n",
    "        labels_tensor = torch.empty(imgs_ct, dtype=int)\n",
    "       \n",
    "        for i in range(imgs_ct):\n",
    "            img = im.open(data_dict['image_path'][i]).convert(\"RGB\")\n",
    "            img = self.common_transforms(img)\n",
    "            imgs_tensor[i] = img\n",
    "            labels_tensor[i] = data_dict['label'][i]\n",
    "        \n",
    "        if stage == 'fit':\n",
    "            #self.mean, self.std = self.compute_mean_std(imgs_tensor)\n",
    "            #imgs_tensor = transforms.functional.normalize(imgs_tensor, self.mean, self.std)\n",
    "            final_dataset = tuple(map(lambda x, y: (x, y,), imgs_tensor, labels_tensor))\n",
    "            train_split = torch.ceil(torch.tensor(imgs_ct * self.train_split)).to(int).item()\n",
    "            self.train_set, self.val_set = random_split(final_dataset, [train_split, imgs_ct-train_split])\n",
    "        else:\n",
    "            #imgs_tensor = transforms.functional.normalize(imgs_tensor, self.mean, self.std)\n",
    "            self.test_set = tuple(map(lambda x, y: (x, y), imgs_tensor, labels_tensor))\n",
    "            \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    #############################\n",
    "    ## Miscellaneous functions ##\n",
    "    #############################\n",
    "    def compute_mean_std(self, image_tensor):\n",
    "        axes = (0,2,3)\n",
    "        return image_tensor.mean(axis=axes), image_tensor.std(axis=axes)\n",
    "        \n",
    "    def get_img_path_labels(self, stage='fit'):\n",
    "        data_dict = {'image_path':[], 'label':[]}\n",
    "        if stage == 'fit':\n",
    "            dir_ = os.path.join(self.root, 'training', 'training')\n",
    "        else:\n",
    "            dir_ = os.path.join(self.root, 'validation', 'validation')\n",
    "            \n",
    "        for cl in self.metadata['Label']:\n",
    "            class_path = os.path.join(dir_, cl)\n",
    "            for img in os.listdir(class_path):\n",
    "                if img.endswith(\".jpg\") or img.endswith(\".png\"):\n",
    "                    fname = os.path.join(class_path, img)\n",
    "                    data_dict['image_path'].append(fname)\n",
    "                    data_dict['label'].append(int(cl[1]))\n",
    "        \n",
    "        return data_dict\n",
    "\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        metadata = dict()\n",
    "        with open(os.path.join(self.root, 'monkey_labels.txt'), mode ='r') as file:\n",
    "            csvFile = csv.reader(file)            \n",
    "            headers = [r.strip() for r in  next(csvFile)]\n",
    "            \n",
    "            for h in headers:\n",
    "                metadata[h] = []\n",
    "                \n",
    "            for lines in csvFile:\n",
    "                for i in range(len(lines)):\n",
    "                    metadata[headers[i]].append(lines[i].strip())\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d50e7d-5a0b-41ff-8bf4-017450755d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonkeyType(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=3e-4, num_classes=10, input_shape=10):\n",
    "        super().__init__()        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # to log the model architecture on tensorboard\n",
    "        self.example_input_array = torch.rand((1, 3, self.hparams.input_shape, self.hparams.input_shape))\n",
    "        \n",
    "        ### Pretrained Model\n",
    "        backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        num_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        # use the pretrained model to classify 10 image classes\n",
    "        self.classifier = nn.Linear(num_filters, self.hparams.num_classes)\n",
    "        \n",
    "        # Model Metrics for logging\n",
    "        acc_obj = Accuracy(num_classes=self.hparams.num_classes)\n",
    "        # use .clone() so that each metric can maintain its own state\n",
    "        self.train_acc = acc_obj.clone()\n",
    "        self.val_acc = acc_obj.clone()\n",
    "        \n",
    "        loss_obj = MeanMetric()\n",
    "        self.train_loss = loss_obj.clone()\n",
    "        self.val_loss = loss_obj.clone()\n",
    "        # End Model Metrics for logging\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        super().on_train_epoch_start()\n",
    "\n",
    "        # Reset state variables for train metrics to \n",
    "        # their default values before start of each epoch\n",
    "        self.train_acc.reset()\n",
    "        self.train_loss.reset()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        pred = self(imgs)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "        pred_classes = probs.argmax(dim=1)\n",
    "        \n",
    "        # accumulate training accuracy for each batch\n",
    "        acc = self.train_acc(pred_classes, labels)\n",
    "        # accumulate training loss for each batch\n",
    "        self.train_loss(loss)\n",
    "        \n",
    "        self.log(\"train/batch_loss\", loss, prog_bar=False)\n",
    "        self.log(\"train/batch_acc\", acc, prog_bar=False)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        avg_train_acc = self.train_acc.compute()\n",
    "        avg_train_loss = self.train_loss.compute()\n",
    "        self.log(\"train/loss\", avg_train_loss, prog_bar=True)\n",
    "        self.log(\"train/acc\", avg_train_acc, prog_bar=True)\n",
    "        # Set X-axis as epoch number for epoch-level metrics\n",
    "        self.log(\"step\", float(self.current_epoch))\n",
    "        \n",
    "    def on_validation_epoch_start(self):\n",
    "        super().on_validation_epoch_start()\n",
    "\n",
    "        # Reset state variables for validation metrics to \n",
    "        # their default values before start of each epoch\n",
    "\n",
    "        self.val_acc.reset()\n",
    "        self.val_loss.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        output = self(imgs)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        pred_classes = prob.argmax(dim=1)\n",
    "        \n",
    "        # accumulate validation accuracy for each batch\n",
    "        self.val_acc(pred_classes, labels)\n",
    "        self.val_loss(loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        avg_val_loss = self.val_loss.compute()\n",
    "        avg_val_acc = self.val_acc.compute()\n",
    "        self.log('val/loss', avg_val_loss, prog_bar=True)\n",
    "        self.log('val/acc', avg_val_acc, prog_bar=True)\n",
    "        self.log(\"step\", float(self.current_epoch))\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        pred = self(imgs)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        prob = F.softmax(pred, dim=1)\n",
    "        pred_classes = prob.argmax(dim=1)\n",
    "        test_acc = torch.where(pred_classes == labels, 1, 0).to(torch.float32).mean()\n",
    "        self.log('test/loss', loss)\n",
    "        self.log('test/acc', test_acc)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71486e9-3c3f-4f4b-9298-e5c40a9fde2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(10, workers=True)\n",
    "image_size = 224\n",
    "m = MonkeyType(learning_rate=0.00001, input_shape=image_size)\n",
    "d = PLDataModule('./../monkeys/', img_size=image_size, batch_size=32, \n",
    "                 train_split=0.80, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9aab32d-aef1-48cc-88ef-d51cfb98f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\", log_graph=True, default_hp_metric=False)\n",
    "trainer = pl.Trainer(accelerator='cpu',\n",
    "                     devices=1,\n",
    "                     fast_dev_run=False,\n",
    "                     enable_progress_bar=True,\n",
    "                     enable_checkpointing=False,\n",
    "                     enable_model_summary=True,\n",
    "                     deterministic=True,\n",
    "                     logger=logger,\n",
    "                     log_every_n_steps=5,\n",
    "                     max_epochs=20,\n",
    "                     check_val_every_n_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256c3500-3bcb-4751-a231-7c0c372c98e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params | In sizes         | Out sizes     \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | feature_extractor | Sequential | 11.2 M | [1, 3, 224, 224] | [1, 512, 1, 1]\n",
      "1 | classifier        | Linear     | 5.1 K  | [1, 512]         | [1, 10]       \n",
      "2 | train_acc         | Accuracy   | 0      | ?                | ?             \n",
      "3 | val_acc           | Accuracy   | 0      | ?                | ?             \n",
      "4 | train_loss        | MeanMetric | 0      | ?                | ?             \n",
      "5 | val_loss          | MeanMetric | 0      | ?                | ?             \n",
      "-------------------------------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46461c0b6ea41fe9a4dabeb76d92a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(m, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f53570-ffa2-4835-a650-121cef60149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836f3843b4584b1fabd2c0704f86373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.25999999046325684\n",
      "        test/loss            2.237553119659424\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 2.237553119659424, 'test/acc': 0.25999999046325684}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(m, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a5018-8255-44e7-9f5b-b7f64195f0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
